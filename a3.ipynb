{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Read the data"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:34:37.819665Z","iopub.status.busy":"2023-05-16T06:34:37.818597Z","iopub.status.idle":"2023-05-16T06:34:51.401178Z","shell.execute_reply":"2023-05-16T06:34:51.399867Z","shell.execute_reply.started":"2023-05-16T06:34:37.819615Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: lightning in /opt/conda/lib/python3.7/site-packages (1.9.5)\n","Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.14.0)\n","Requirement already satisfied: lightning-cloud>=0.5.27 in /opt/conda/lib/python3.7/site-packages (from lightning) (0.5.36)\n","Requirement already satisfied: torch<4.0,>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (1.13.0)\n","Requirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (4.64.1)\n","Requirement already satisfied: starlette<2.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (0.22.0)\n","Requirement already satisfied: urllib3<3.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (1.26.14)\n","Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (4.4.0)\n","Requirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from lightning) (1.21.6)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from lightning) (23.0)\n","Requirement already satisfied: dateutils<2.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (0.6.12)\n","Requirement already satisfied: lightning-utilities<2.0,>=0.6.0.post0 in /opt/conda/lib/python3.7/site-packages (from lightning) (0.8.0)\n","Requirement already satisfied: croniter<1.4.0,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (1.3.14)\n","Requirement already satisfied: starsessions<2.0,>=1.2.1 in /opt/conda/lib/python3.7/site-packages (from lightning) (1.3.0)\n","Requirement already satisfied: psutil<7.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (5.9.3)\n","Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (4.11.1)\n","Requirement already satisfied: pydantic<3.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (1.10.4)\n","Requirement already satisfied: deepdiff<8.0,>=5.7.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (6.3.0)\n","Requirement already satisfied: traitlets<7.0,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (5.8.1)\n","Requirement already satisfied: fastapi<0.89.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (0.88.0)\n","Requirement already satisfied: click<10.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (8.1.3)\n","Requirement already satisfied: PyYAML<8.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (6.0)\n","Requirement already satisfied: fsspec<2024.0,>=2022.5.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (2023.1.0)\n","Requirement already satisfied: torchmetrics<2.0,>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (0.11.4)\n","Requirement already satisfied: uvicorn<2.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (0.20.0)\n","Requirement already satisfied: arrow<3.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (1.2.3)\n","Requirement already satisfied: Jinja2<5.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (3.1.2)\n","Requirement already satisfied: rich<15.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (13.2.0)\n","Requirement already satisfied: websocket-client<3.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (1.4.2)\n","Requirement already satisfied: inquirer<5.0,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (2.10.1)\n","Requirement already satisfied: requests<4.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (2.28.2)\n","Requirement already satisfied: websockets<12.0 in /opt/conda/lib/python3.7/site-packages (from lightning) (11.0)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.20.3)\n","Requirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\n","Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.30)\n","Requirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.3.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.4.4)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.18.0)\n","Requirement already satisfied: python-dateutil>=2.7.0 in /opt/conda/lib/python3.7/site-packages (from arrow<3.0,>=1.2.0->lightning) (2.8.2)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4<6.0,>=4.8.0->lightning) (2.3.2.post1)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<10.0->lightning) (4.11.4)\n","Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from dateutils<2.0->lightning) (2023.3)\n","Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /opt/conda/lib/python3.7/site-packages (from deepdiff<8.0,>=5.7.0->lightning) (4.1.0)\n","Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from starlette<2.0->lightning) (3.6.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.7/site-packages (from fsspec<2024.0,>=2022.5.0->lightning) (3.8.3)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n","Requirement already satisfied: readchar>=3.0.6 in /opt/conda/lib/python3.7/site-packages (from inquirer<5.0,>=2.10.0->lightning) (4.0.5)\n","Requirement already satisfied: blessed>=1.19.0 in /opt/conda/lib/python3.7/site-packages (from inquirer<5.0,>=2.10.0->lightning) (1.19.1)\n","Requirement already satisfied: python-editor>=1.0.4 in /opt/conda/lib/python3.7/site-packages (from inquirer<5.0,>=2.10.0->lightning) (1.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from Jinja2<5.0->lightning) (2.1.1)\n","Requirement already satisfied: pyjwt in /opt/conda/lib/python3.7/site-packages (from lightning-cloud>=0.5.27->lightning) (2.6.0)\n","Requirement already satisfied: python-multipart in /opt/conda/lib/python3.7/site-packages (from lightning-cloud>=0.5.27->lightning) (0.0.6)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<4.0->lightning) (3.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<4.0->lightning) (2.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<4.0->lightning) (2022.12.7)\n","Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from rich<15.0->lightning) (2.1.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from rich<15.0->lightning) (2.14.0)\n","Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from starsessions<2.0,>=1.2.1->lightning) (2.1.2)\n","Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.7/site-packages (from uvicorn<2.0->lightning) (0.14.0)\n","Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (0.13.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (1.3.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (1.8.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (22.2.0)\n","Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.7/site-packages (from anyio<5,>=3.4.0->starlette<2.0->lightning) (1.3.0)\n","Requirement already satisfied: wcwidth>=0.1.4 in /opt/conda/lib/python3.7/site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (0.2.6)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click<10.0->lightning) (3.11.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.7/site-packages (from markdown-it-py<3.0.0,>=2.1.0->rich<15.0->lightning) (0.1.2)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install lightning wandb"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:34:51.405632Z","iopub.status.busy":"2023-05-16T06:34:51.404518Z","iopub.status.idle":"2023-05-16T06:34:52.413179Z","shell.execute_reply":"2023-05-16T06:34:52.411683Z","shell.execute_reply.started":"2023-05-16T06:34:51.405581Z"},"trusted":true},"outputs":[],"source":["!WANDB_API_KEY=8c780297be240a84f5c8b7d669cb158839b2637a"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:34:52.417245Z","iopub.status.busy":"2023-05-16T06:34:52.416363Z","iopub.status.idle":"2023-05-16T06:34:52.424292Z","shell.execute_reply":"2023-05-16T06:34:52.423163Z","shell.execute_reply.started":"2023-05-16T06:34:52.417197Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import torch \n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import lightning as pl\n","from pytorch_lightning.loggers import WandbLogger\n","import random\n","import wandb"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:34:52.430470Z","iopub.status.busy":"2023-05-16T06:34:52.429462Z","iopub.status.idle":"2023-05-16T06:34:55.047539Z","shell.execute_reply":"2023-05-16T06:34:55.046287Z","shell.execute_reply.started":"2023-05-16T06:34:52.430426Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs20b075\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]}],"source":["!WANDB_API_KEY=8c780297be240a84f5c8b7d669cb158839b2637a wandb login"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:36:04.776933Z","iopub.status.busy":"2023-05-16T06:36:04.775718Z","iopub.status.idle":"2023-05-16T06:36:04.784344Z","shell.execute_reply":"2023-05-16T06:36:04.783193Z","shell.execute_reply.started":"2023-05-16T06:36:04.776878Z"},"trusted":true},"outputs":[],"source":["path = \"aksharantar_sampled/hin\"\n","train_path = path + \"/hin_train.csv\"\n","valid_path = path + \"/hin_valid.csv\"\n","test_path = path + \"/hin_test.csv\""]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:36:05.583197Z","iopub.status.busy":"2023-05-16T06:36:05.581978Z","iopub.status.idle":"2023-05-16T06:36:05.589915Z","shell.execute_reply":"2023-05-16T06:36:05.588474Z","shell.execute_reply.started":"2023-05-16T06:36:05.583147Z"},"trusted":true},"outputs":[],"source":["def get_data(path):\n","    dataset = pd.read_csv(path, header=None)\n","    dataset = dataset.values\n","    input = dataset[:, 0]\n","    output = dataset[:, 1]\n","    return input, output"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:36:06.239945Z","iopub.status.busy":"2023-05-16T06:36:06.239535Z","iopub.status.idle":"2023-05-16T06:36:06.337010Z","shell.execute_reply":"2023-05-16T06:36:06.335895Z","shell.execute_reply.started":"2023-05-16T06:36:06.239908Z"},"trusted":true},"outputs":[],"source":["train_dataset = get_data(train_path)\n","val_dataset = get_data(valid_path)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:36:07.896068Z","iopub.status.busy":"2023-05-16T06:36:07.895677Z","iopub.status.idle":"2023-05-16T06:36:07.905845Z","shell.execute_reply":"2023-05-16T06:36:07.904490Z","shell.execute_reply.started":"2023-05-16T06:36:07.896033Z"},"trusted":true},"outputs":[],"source":["def convert_word_to_tensor(word, lang):\n","    lang_to_int = {'SOS': 0, 'EOS': 1, 'PAD': 2}\n","    if lang == 'eng':\n","        lang_to_int.update({chr(i): i-94 for i in range(97, 123)})\n","    elif lang == 'hin':\n","        lang_to_int.update({chr(i): i-2300 for i in range(2304, 2432)})\n","    \n","    a = [lang_to_int['SOS']]\n","\n","    for i in word:\n","        a.append(lang_to_int[i])\n","\n","    a.append(lang_to_int['EOS'])\n","    if len(a) < 24:\n","        a.extend([lang_to_int['PAD']]*(24-len(a)))\n","    \n","    return torch.tensor(a)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:36:08.647884Z","iopub.status.busy":"2023-05-16T06:36:08.647495Z","iopub.status.idle":"2023-05-16T06:36:08.659827Z","shell.execute_reply":"2023-05-16T06:36:08.658704Z","shell.execute_reply.started":"2023-05-16T06:36:08.647849Z"},"trusted":true},"outputs":[],"source":["class AksharantarDataset(Dataset):\n","    def __init__(self, dataset):\n","        super().__init__()\n","        self.dataset = dataset\n","        self.input = dataset[0]\n","        self.output = dataset[1]\n","        mask = np.array([len(elem) < 21 for elem in self.input]) & np.array([len(elem) < 21 for elem in self.output])\n","        self.input = self.input[mask]\n","        self.output = self.output[mask]\n","        self.len = len(self.input)\n","    \n","    def __getitem__(self, index):\n","        return convert_word_to_tensor(self.input[index], 'eng'), convert_word_to_tensor(self.output[index], 'hin')\n","    \n","    def __len__(self):\n","        return self.len"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:36:09.072136Z","iopub.status.busy":"2023-05-16T06:36:09.071756Z","iopub.status.idle":"2023-05-16T06:36:09.080077Z","shell.execute_reply":"2023-05-16T06:36:09.078868Z","shell.execute_reply.started":"2023-05-16T06:36:09.072103Z"},"trusted":true},"outputs":[],"source":["class CustomDataModule(pl.LightningDataModule):\n","    def __init__(self, dataset, val_dataset, batch_size=32):\n","        super().__init__()\n","        self.dataset = train_dataset\n","        self.val_dataset = val_dataset\n","        self.batch_size = batch_size\n","\n","    def train_dataloader(self):\n","        dataset = AksharantarDataset(self.dataset)\n","        return DataLoader(dataset, batch_size=self.batch_size, num_workers=2)\n","    def val_dataloader(self):\n","        dataset = AksharantarDataset(self.val_dataset)\n","        return DataLoader(dataset, batch_size=self.batch_size, num_workers=2)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:36:09.739374Z","iopub.status.busy":"2023-05-16T06:36:09.738298Z","iopub.status.idle":"2023-05-16T06:36:09.753016Z","shell.execute_reply":"2023-05-16T06:36:09.751525Z","shell.execute_reply.started":"2023-05-16T06:36:09.739322Z"},"trusted":true},"outputs":[],"source":["train_loader = CustomDataModule(train_dataset, val_dataset, 32)\n","# val_loader = CustomDataModule(val_dataset, 32)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Encoder model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:36:11.018191Z","iopub.status.busy":"2023-05-16T06:36:11.017766Z","iopub.status.idle":"2023-05-16T06:36:11.032010Z","shell.execute_reply":"2023-05-16T06:36:11.030663Z","shell.execute_reply.started":"2023-05-16T06:36:11.018152Z"},"trusted":true},"outputs":[],"source":["class Encoder(pl.LightningModule):\n","    def __init__(self, input_size, hidden_size, cell_type, num_layers=1, dropout=0, bidirectional=False):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.cell_type = cell_type\n","        if cell_type == 'LSTM':\n","            self.rnn = nn.LSTM\n","        elif cell_type == 'GRU':\n","            self.rnn = nn.GRU\n","        else:\n","            self.rnn = nn.RNN\n","        self.direction = 2 if bidirectional else 1\n","        self.first_cell = self.rnn(hidden_size, hidden_size, bidirectional=bidirectional, batch_first=True)\n","        self.rnns = nn.ModuleList([self.rnn(hidden_size*self.direction, hidden_size, bidirectional=bidirectional, batch_first=True)]*(num_layers-1))\n","        self.num_layers = num_layers\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input)\n","        # embedded = embedded.view(1, 1, -1)\n","        output = embedded\n","        output, hidden = self.first_cell(output, hidden)\n","        for i in range(self.num_layers-1):\n","            output, hidden = self.rnns[i](output, hidden)\n","        return output, hidden\n","\n","    def init_hidden(self):\n","        if self.cell_type == 'LSTM':\n","            return torch.zeros(self.direction, self.hidden_size), torch.zeros(self.direction, self.hidden_size)\n","        return torch.zeros(self.direction, self.hidden_size, device=self.device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Decoder"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:36:11.637653Z","iopub.status.busy":"2023-05-16T06:36:11.637209Z","iopub.status.idle":"2023-05-16T06:36:11.652484Z","shell.execute_reply":"2023-05-16T06:36:11.651078Z","shell.execute_reply.started":"2023-05-16T06:36:11.637615Z"},"trusted":true},"outputs":[],"source":["class Decoder(pl.LightningModule):\n","    def __init__(self, output_size, hidden_size, cell_type, num_layers=1, bidirectional=False, dropout=0):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        if cell_type == 'LSTM':\n","            self.cell_type = nn.LSTM\n","        elif cell_type == 'GRU':\n","            self.cell_type = nn.GRU\n","        else:\n","            self.cell_type = nn.RNN\n","        self.first_cell = self.cell_type(hidden_size, hidden_size, bidirectional=bidirectional, batch_first=True)\n","        self.direction = 2 if bidirectional else 1\n","        self.rnns= nn.ModuleList([self.cell_type(hidden_size*self.direction, hidden_size, bidirectional=bidirectional, batch_first=True)]*(num_layers-1))\n","        self.out = nn.Linear(hidden_size*self.direction, output_size)\n","        self.softmax = nn.LogSoftmax(dim=-1)\n","        self.num_layers = num_layers\n","\n","    def forward(self, input, hidden):\n","        output = self.embedding(input)\n","        output = nn.functional.relu(output)\n","        output, hidden = self.first_cell(output, hidden)\n","        for i in range(self.num_layers-1):\n","            output, hidden = self.rnns[i](output, hidden)\n","        linear_output = self.out(output)\n","        output = self.softmax(self.out(output))\n","        if output.shape[0] == 1:\n","            output = output.squeeze(0)\n","        return output, hidden"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Seq2seq model"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:46:55.482586Z","iopub.status.busy":"2023-05-16T06:46:55.482120Z","iopub.status.idle":"2023-05-16T06:46:55.549975Z","shell.execute_reply":"2023-05-16T06:46:55.548793Z","shell.execute_reply.started":"2023-05-16T06:46:55.482547Z"},"trusted":true},"outputs":[],"source":["class Seq2seq(pl.LightningModule):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder.to(self.device)\n","        self.decoder = decoder.to(self.device)\n","\n","    def forward(self, input):\n","        \n","        self.encoder = self.encoder.to(self.device)\n","        self.decoder = self.decoder.to(self.device)\n","        \n","        batched = True if len(input.shape) > 1 else False\n","        if not batched:\n","            input = input.unsqueeze(0)\n","        input = input.to(self.device)\n","        batch_size = input.shape[0]\n","        input_length = input.shape[1]\n","\n","        encoder_hidden = None\n","        encoder_hidden_outputs = torch.zeros(input_length, self.encoder.direction, batch_size, self.encoder.hidden_size, device=self.device)\n","        encoder_output_gate = torch.zeros(input_length, self.encoder.direction, batch_size, self.encoder.hidden_size, device=self.device)\n","        if self.encoder.cell_type == 'LSTM':\n","            a, b = [torch.zeros(self.encoder.direction, batch_size, self.encoder.hidden_size)]*2\n","            encoder_hidden = a.to(self.device), b.to(self.device)\n","        else:\n","            encoder_hidden = torch.zeros(self.encoder.direction, batch_size, self.encoder.hidden_size).to(self.device)\n","        for i in range(input_length):\n","            # print(input[:, i].shape, encoder_hidden.shape)\n","            _, encoder_hidden_out = self.encoder(input[:, i].unsqueeze(1), encoder_hidden)\n","            encoder_hidden = encoder_hidden_out\n","            if self.encoder.cell_type == 'LSTM':\n","                encoder_hidden_outputs[i] = encoder_hidden_out[0]\n","                encoder_output_gate[i] = encoder_hidden_out[1]\n","            else:\n","                encoder_hidden_outputs[i] = encoder_hidden_out\n","        if self.encoder.cell_type == 'LSTM':\n","            decoder_hidden = encoder_hidden_outputs[-1], encoder_output_gate[-1]\n","        else:\n","            decoder_hidden = encoder_hidden_outputs[-1]\n","        decoder_input = torch.zeros(batch_size, 1, dtype=torch.long, device=self.device)\n","        for j in range(input_length):\n","            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n","            decoder_input = decoder_output.argmax(dim=-1)\n","        output_sequence = torch.tensor(output_sequence, device=self.device)\n","        if not batched:\n","            output_sequence = output_sequence.squeeze(0)\n","        return output_sequence\n","        \n","    def training_step(self, batch, batch_idx):\n","        input, target = batch\n","        \n","        self.encoder = self.encoder.to(self.device)\n","        self.decoder = self.decoder.to(self.device)\n","        \n","        batched = True if len(input.shape) > 1 else False\n","        if not batched:\n","            input = input.unsqueeze(0)\n","            target = target.unsqueeze(0)\n","        input = input.to(self.device)\n","        target = target.to(self.device)\n","        batch_size = input.shape[0]\n","        input_length = input.shape[1]\n","        target_length = target.shape[1]\n","\n","        encoder_hidden = None\n","        encoder_hidden_outputs = torch.zeros(input_length, self.encoder.direction, batch_size, self.encoder.hidden_size, device=self.device)\n","        encoder_output_gate = torch.zeros(input_length, self.encoder.direction, batch_size, self.encoder.hidden_size, device=self.device)\n","        if self.encoder.cell_type == 'LSTM':\n","            a, b = [torch.zeros(self.encoder.direction, batch_size, self.encoder.hidden_size)]*2\n","            encoder_hidden = a.to(self.device), b.to(self.device)\n","        else:\n","            encoder_hidden = torch.zeros(self.encoder.direction, batch_size, self.encoder.hidden_size).to(self.device)\n","        for i in range(input_length):\n","            # print(input[:, i].shape, encoder_hidden.shape)\n","            _, encoder_hidden_out = self.encoder(input[:, i].unsqueeze(1), encoder_hidden)\n","            encoder_hidden = encoder_hidden_out\n","            if self.encoder.cell_type == 'LSTM':\n","                encoder_hidden_outputs[i] = encoder_hidden_out[0]\n","                encoder_output_gate[i] = encoder_hidden_out[1]\n","            else:\n","                encoder_hidden_outputs[i] = encoder_hidden_out\n","        loss = 0\n","        correct_words = 0\n","        if self.encoder.cell_type == 'LSTM':\n","            decoder_hidden = encoder_hidden_outputs[-1], encoder_output_gate[-1]\n","        else:\n","            decoder_hidden = encoder_hidden_outputs[-1]\n","        if random.random() < 0.5: \n","            decoder_input = target[:, 0].unsqueeze(1)\n","            correct = None\n","            for j in range(target_length):\n","                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n","                squeezed_output = decoder_output.squeeze(1)\n","                for i in range(batch_size):\n","                    loss += nn.functional.nll_loss(squeezed_output[i], target[i, j])\n","                decoder_input = target[:, j].unsqueeze(1)\n","                if correct is None:\n","                    correct = decoder_output.argmax(dim=-1) == target[:, j]\n","                else:\n","                    correct = (decoder_output.argmax(dim=-1) == target[:, j]) & correct\n","            correct_words = correct.sum()\n","\n","        else:\n","            decoder_input = target[:, 0].unsqueeze(1)\n","            correct = None\n","            for j in range(target_length):\n","                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n","                squeezed_output = decoder_output.squeeze(1)\n","                for i in range(batch_size):\n","                    loss += nn.functional.nll_loss(squeezed_output[i], target[i, j])\n","                decoder_input = decoder_output.argmax(dim=-1)\n","                if correct is None:\n","                    correct = decoder_input == target[:, j]\n","                else:\n","                    correct = (decoder_input == target[:, j]) & correct\n","            correct_words = correct.sum()\n","\n","        # print(correct_words, batch_size, correct_words/batch_size)\n","        reported_loss = loss / (batch_size * target_length)\n","        self.log('train_loss', reported_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('train_acc', correct_words/batch_size, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n","        return loss\n","    def validation_step(self, batch, batch_idx):\n","        input, target = batch\n","        \n","        self.encoder = self.encoder.to(self.device)\n","        self.decoder = self.decoder.to(self.device)\n","        \n","        batched = True if len(input.shape) > 1 else False\n","        if not batched:\n","            input = input.unsqueeze(0)\n","            target = target.unsqueeze(0)\n","        input = input.to(self.device)\n","        target = target.to(self.device)\n","        batch_size = input.shape[0]\n","        input_length = input.shape[1]\n","        target_length = target.shape[1]\n","\n","        encoder_hidden = None\n","        encoder_hidden_outputs = torch.zeros(input_length, self.encoder.direction, batch_size, self.encoder.hidden_size, device=self.device)\n","        encoder_output_gate = torch.zeros(input_length, self.encoder.direction, batch_size, self.encoder.hidden_size, device=self.device)\n","        if self.encoder.cell_type == 'LSTM':\n","            a, b = [torch.zeros(self.encoder.direction, batch_size, self.encoder.hidden_size)]*2\n","            encoder_hidden = a.to(self.device), b.to(self.device)\n","        else:\n","            encoder_hidden = torch.zeros(self.encoder.direction, batch_size, self.encoder.hidden_size).to(self.device)\n","        for i in range(input_length):\n","            # print(input[:, i].shape, encoder_hidden.shape)\n","            _, encoder_hidden_out = self.encoder(input[:, i].unsqueeze(1), encoder_hidden)\n","            encoder_hidden = encoder_hidden_out\n","            if self.encoder.cell_type == 'LSTM':\n","                encoder_hidden_outputs[i] = encoder_hidden_out[0]\n","                encoder_output_gate[i] = encoder_hidden_out[1]\n","            else:\n","                encoder_hidden_outputs[i] = encoder_hidden_out\n","        loss = 0\n","        correct_words = 0\n","        if self.encoder.cell_type == 'LSTM':\n","            decoder_hidden = encoder_hidden_outputs[-1], encoder_output_gate[-1]\n","        else:\n","            decoder_hidden = encoder_hidden_outputs[-1]\n","        decoder_input = target[:, 0].unsqueeze(1)\n","        correct = None\n","        for j in range(target_length):\n","            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n","            squeezed_output = decoder_output.squeeze(1)\n","            for i in range(batch_size):\n","                loss += nn.functional.nll_loss(squeezed_output[i], target[i, j])\n","            decoder_input = decoder_output.argmax(dim=-1)\n","            if correct is None:\n","                correct = decoder_input == target[:, j]\n","            else:\n","                correct = (decoder_input == target[:, j]) & correct\n","        correct_words = correct.sum()\n","\n","        # for i in range(batch_size):\n","        #     if self.encoder.cell_type == 'LSTM':\n","        #         decoder_hidden = encoder_hidden_outputs[i].view(self.decoder.direction, -1), encoder_output_gate[i].view(self.decoder.direction, -1)\n","        #     else:\n","        #         decoder_hidden = encoder_hidden_outputs[i].view(self.decoder.direction, -1)\n","        #     decoder_input = target[i, 0].unsqueeze(0)\n","        #     correct = True\n","        #     for j in range(target_length):\n","        #         decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n","        #         loss += nn.functional.nll_loss(decoder_output, target[i, j])\n","        #         decoder_input = torch.tensor([decoder_output.argmax().item()]).to(self.device)\n","        #         if correct and target[i, j]!= decoder_output.argmax().item():\n","        #             correct = False\n","        #     if correct:\n","        #         correct_words  += 1\n","        reported_loss = loss / (batch_size * target_length)\n","        self.log('val_loss', reported_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('val_acc', correct_words/batch_size, on_epoch=True, prog_bar=True, logger=True)\n","        return loss\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T06:46:55.861327Z","iopub.status.busy":"2023-05-16T06:46:55.860979Z","iopub.status.idle":"2023-05-16T06:46:55.882562Z","shell.execute_reply":"2023-05-16T06:46:55.881535Z","shell.execute_reply.started":"2023-05-16T06:46:55.861294Z"},"trusted":true},"outputs":[],"source":["encoder = Encoder(30, 64, cell_type=\"LSTM\", num_layers=2, bidirectional=True, dropout=0.1)\n","decoder = Decoder(150, 64, cell_type=\"LSTM\", num_layers=3, bidirectional=True)\n","model = Seq2seq(encoder, decoder)"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","\n","  | Name    | Type    | Params\n","------------------------------------\n","0 | encoder | Encoder | 167 K \n","1 | decoder | Decoder | 194 K \n","------------------------------------\n","362 K     Trainable params\n","0         Non-trainable params\n","362 K     Total params\n","1.451     Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0:   0%|          | 0/1599 [13:38<?, ?it/s] 4.61it/s, v_num=186, train_loss_step=1.220, train_acc_step=0.000]\n","Epoch 0:   0%|          | 2/1599 [12:46<170:02:08, 383.30s/it, v_num=175, train_loss_step=4.800, train_acc_step=0.000]\n","Epoch 0:   0%|          | 2/1599 [12:14<162:51:33, 367.12s/it, v_num=176, train_loss_step=4.780, train_acc_step=0.000]\n","Epoch 0:   0%|          | 0/1599 [09:47<?, ?it/s]\n","Epoch 0:   0%|          | 1/1599 [08:56<238:04:40, 536.35s/it, v_num=178, train_loss_step=4.980, train_acc_step=0.000]\n","Epoch 0:   0%|          | 0/1599 [05:13<?, ?it/s]\n","Epoch 0:   0%|          | 0/1599 [04:55<?, ?it/s]\n","Epoch 0: 100%|██████████| 1599/1599 [06:32<00:00,  4.07it/s, v_num=186, train_loss_step=1.060, train_acc_step=0.000, val_loss_step=1.150, val_loss_epoch=1.200, val_acc=0.000, train_loss_epoch=1.310, train_acc_epoch=0.000]"]},{"name":"stderr","output_type":"stream","text":["`Trainer.fit` stopped: `max_epochs=1` reached.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0: 100%|██████████| 1599/1599 [06:32<00:00,  4.07it/s, v_num=186, train_loss_step=1.060, train_acc_step=0.000, val_loss_step=1.150, val_loss_epoch=1.200, val_acc=0.000, train_loss_epoch=1.310, train_acc_epoch=0.000]\n"]}],"source":["trainer = pl.Trainer(max_epochs=1)\n","trainer.fit(model, train_loader)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T20:24:54.863147Z","iopub.status.busy":"2023-05-11T20:24:54.862722Z","iopub.status.idle":"2023-05-11T20:24:54.871728Z","shell.execute_reply":"2023-05-11T20:24:54.870454Z","shell.execute_reply.started":"2023-05-11T20:24:54.863108Z"},"trusted":true},"outputs":[],"source":["def convert_tensor_to_word(tensor, lang):\n","    int_to_lang = {0: 'SOS', 1: 'EOS', 2: 'PAD'}\n","    if lang == 'eng':\n","        int_to_lang.update({i-94: chr(i) for i in range(97, 123)})\n","    elif lang == 'hin':\n","        int_to_lang.update({i-2300: chr(i) for i in range(2304, 2432)})\n","    \n","    word = ''\n","    for i in tensor:\n","        word += int_to_lang[i.item()]\n","    return word"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"ename":"UnboundLocalError","evalue":"cannot access local variable 'target' where it is not associated with a value","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m convert_tensor_to_word(model(convert_word_to_tensor(\u001b[39m'\u001b[39;49m\u001b[39mgharelu\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39meng\u001b[39;49m\u001b[39m'\u001b[39;49m)), \u001b[39m'\u001b[39m\u001b[39mhin\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[14], line 15\u001b[0m, in \u001b[0;36mAttnSeq2seq.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched:\n\u001b[1;32m     14\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n","\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'target' where it is not associated with a value"]}],"source":["convert_tensor_to_word(model(convert_word_to_tensor('gharelu', 'eng')), 'hin')"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T20:24:55.521250Z","iopub.status.busy":"2023-05-11T20:24:55.519868Z","iopub.status.idle":"2023-05-11T20:24:55.529506Z","shell.execute_reply":"2023-05-11T20:24:55.528393Z","shell.execute_reply.started":"2023-05-11T20:24:55.521200Z"},"trusted":true},"outputs":[],"source":["sweep_config = {\n","    'method': 'bayes',\n","    'metric': {\n","        'name': 'val_loss',\n","        'goal': 'minimize'\n","    },\n","    'parameters': {\n","        'hidden_size': {\n","            'values': [64, 128, 256],\n","        },\n","        'encoder_num_layers': {\n","            'values': [1, 2, 3],\n","        },\n","        'decoder_num_layers': {\n","            'values': [1, 2, 3],\n","        },\n","        'bidirectional': {\n","            'values': [True, False],\n","        },\n","        'cell_type': {\n","            'values': ['LSTM', 'GRU'],\n","        },\n","    }\n","}"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T20:24:56.814281Z","iopub.status.busy":"2023-05-11T20:24:56.812905Z","iopub.status.idle":"2023-05-11T20:24:56.823454Z","shell.execute_reply":"2023-05-11T20:24:56.822264Z","shell.execute_reply.started":"2023-05-11T20:24:56.814223Z"},"trusted":true},"outputs":[],"source":["def sweep_fn():\n","    wandb.init()\n","    config = wandb.config\n","    dropout_val = 0\n","    encoder=Encoder(30, config.hidden_size, config.cell_type, num_layers=config.encoder_num_layers, bidirectional=config.bidirectional)\n","    decoder = Decoder(150, config.hidden_size, config.cell_type, num_layers=config.decoder_num_layers, bidirectional=config.bidirectional)\n","    model = Seq2seq(encoder, decoder)\n","    logger = WandbLogger(project='CS6910 Assignment 3', entity='cs20b075')\n","    trainer = pl.Trainer(max_epochs=5, precision=16, logger=logger)\n","    trainer.fit(model, train_loader)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T20:24:59.204864Z","iopub.status.busy":"2023-05-11T20:24:59.204190Z","iopub.status.idle":"2023-05-11T20:24:59.305703Z","shell.execute_reply":"2023-05-11T20:24:59.304455Z","shell.execute_reply.started":"2023-05-11T20:24:59.204820Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs20b075\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/sooraj/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login(key=\"8c780297be240a84f5c8b7d669cb158839b2637a\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T20:25:02.475513Z","iopub.status.busy":"2023-05-11T20:25:02.474451Z","iopub.status.idle":"2023-05-11T20:29:19.350116Z","shell.execute_reply":"2023-05-11T20:29:19.346270Z","shell.execute_reply.started":"2023-05-11T20:25:02.475451Z"},"trusted":true},"outputs":[],"source":["sweep_id = wandb.sweep(sweep=sweep_config, project=\"CS6910 Assignment 3\")\n","wandb.agent(sweep_id=sweep_id, function=sweep_fn, count=10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wandb.agent(sweep_id=\"1aw4o8ik\", function=sweep_fn, count=10, project=\"CS6910 Assignment 3\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T20:19:23.845801Z","iopub.status.busy":"2023-05-11T20:19:23.841384Z","iopub.status.idle":"2023-05-11T20:19:23.852667Z","shell.execute_reply":"2023-05-11T20:19:23.850824Z","shell.execute_reply.started":"2023-05-11T20:19:23.845753Z"},"trusted":true},"outputs":[],"source":["wandb.finish()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Adding attention to the Seq2Seq model"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["class AttnDecoder(pl.LightningModule):\n","    def __init__(self, output_size, hidden_size, attention_size, cell_type, num_layers=1, bidirectional=False, dropout=0):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        if cell_type == 'LSTM':\n","            self.cell_type = nn.LSTM\n","        elif cell_type == 'GRU':\n","            self.cell_type = nn.GRU\n","        else:\n","            self.cell_type = nn.RNN\n","        self.first_cell = self.cell_type(hidden_size, hidden_size, bidirectional=bidirectional, batch_first=True)\n","        self.direction = 2 if bidirectional else 1\n","        self.rnns= nn.ModuleList([self.cell_type(hidden_size*self.direction, hidden_size, bidirectional=bidirectional, batch_first=True)]*(num_layers-1))\n","        self.out = nn.Linear(hidden_size*self.direction, output_size)\n","        self.softmax = nn.LogSoftmax(dim=-1)\n","        self.num_layers = num_layers\n","\n","        self.Uattn = nn.Linear(hidden_size*self.direction, attention_size)\n","        self.Wattn = nn.Linear(hidden_size*self.direction, attention_size)\n","        self.Vattn = nn.Linear(attention_size, 1)\n","\n","        self.attn_combine = nn.Linear(hidden_size + hidden_size*self.direction, hidden_size)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","        # print(\"Am in the decoder\")\n","        # print(\"Printing the shapes of everything here:\")\n","        # print(\"Input shape:\", input.shape)\n","        # print(\"Hidden shape:\", hidden.shape)\n","        # print(\"Encoder outputs shape:\", encoder_outputs.shape)\n","        encoder_outputs_flat = encoder_outputs.transpose(1, 2).flatten(2)\n","        hidden_flat = None\n","        if self.cell_type == nn.LSTM:\n","            hidden_flat = hidden[0].transpose(0, 1).flatten(1)\n","        else:\n","            hidden_flat = hidden.transpose(0, 1).flatten(1)\n","        # print(\"Flattened shapes:\", encoder_outputs_flat.shape, hidden_flat.shape)\n","        # print(\"HIdden shapes:\", self.Uattn.shape, self.Wattn.shape, self.Vattn.shape)\n","        encoder_part = self.Uattn(encoder_outputs_flat)\n","        # print(\"got past Uattn\", encoder_outputs_flat.shape, encoder_part.shape)\n","        decoder_part = self.Wattn(hidden_flat.repeat(encoder_outputs.shape[0], 1, 1))\n","        # print(\"got past Wattn\", decoder_part.shape)\n","        # ejt = torch.tanh(self.Uattn(encoder_outputs) + self.Wattn(hidden[0].repeat(1, encoder_outputs.shape[1], 1)))\n","        ejt = torch.tanh(encoder_part + decoder_part)\n","        at = self.Vattn(ejt).squeeze(-1)\n","        # print(at.shape)\n","        at = at.transpose(0, 1).unsqueeze(1)\n","        at = nn.functional.softmax(at, dim=-1)\n","        # print(\"Attention\", at.shape)\n","        encoder_outputs_flat = encoder_outputs_flat.transpose(0, 1)\n","        # print(\"Encoder outputs flat\", encoder_outputs_flat.shape)\n","        context = torch.bmm(at, encoder_outputs_flat).squeeze(1)\n","        # print(\"Context\", context.shape)\n","        \n","        output = self.embedding(input)\n","        # print(\"Output\", output.shape)\n","        output = nn.functional.relu(output)\n","        output = torch.cat((output.squeeze(1), context), dim=-1).unsqueeze(1)\n","        output = self.attn_combine(output)\n","        output, hidden = self.first_cell(output, hidden)\n","        for i in range(self.num_layers-1):\n","            output, hidden = self.rnns[i](output, hidden)\n","        linear_output = self.out(output)\n","        output = self.softmax(self.out(output))\n","        if output.shape[0] == 1:\n","            output = output.squeeze(0)\n","        return output, hidden, at"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["class AttnSeq2seq(pl.LightningModule):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder.to(self.device)\n","        self.decoder = decoder.to(self.device)\n","\n","    def forward(self, input):\n","        \n","        self.encoder = self.encoder.to(self.device)\n","        self.decoder = self.decoder.to(self.device)\n","        \n","        batched = True if len(input.shape) > 1 else False\n","        if not batched:\n","            input = input.unsqueeze(0)\n","        input = input.to(self.device)\n","        batch_size = input.shape[0]\n","        input_length = input.shape[1]\n","\n","        encoder_hidden = None\n","        encoder_hidden_outputs = torch.zeros(input_length, self.encoder.direction, batch_size, self.encoder.hidden_size, device=self.device)\n","        encoder_output_gate = torch.zeros(input_length, self.encoder.direction, batch_size, self.encoder.hidden_size, device=self.device)\n","        if self.encoder.cell_type == 'LSTM':\n","            a, b = [torch.zeros(self.encoder.direction, batch_size, self.encoder.hidden_size)]*2\n","            encoder_hidden = a.to(self.device), b.to(self.device)\n","        else:\n","            encoder_hidden = torch.zeros(self.encoder.direction, batch_size, self.encoder.hidden_size).to(self.device)\n","        for i in range(input_length):\n","            # print(input[:, i].shape, encoder_hidden.shape)\n","            _, encoder_hidden_out = self.encoder(input[:, i].unsqueeze(1), encoder_hidden)\n","            encoder_hidden = encoder_hidden_out\n","            if self.encoder.cell_type == 'LSTM':\n","                encoder_hidden_outputs[i] = encoder_hidden_out[0]\n","                encoder_output_gate[i] = encoder_hidden_out[1]\n","            else:\n","                encoder_hidden_outputs[i] = encoder_hidden_out\n","        if self.encoder.cell_type == 'LSTM':\n","            decoder_hidden = encoder_hidden_outputs[-1], encoder_output_gate[-1]\n","        else:\n","            decoder_hidden = encoder_hidden_outputs[-1]\n","        decoder_input = torch.zeros(batch_size, 1, dtype=torch.long, device=self.device)\n","        output_sequences = []\n","        attention_weights = [] \n","        for j in range(24):\n","            decoder_output, decoder_hidden, at = self.decoder(decoder_input, decoder_hidden, encoder_hidden_outputs)\n","            decoder_input = decoder_output.argmax(dim=-1)\n","            output_sequences.append(decoder_input.item ())\n","            attention_weights.append(at)\n","        output_sequence = torch.tensor(output_sequences, device=self.device)\n","        # attention_weights = torch.tensor(attention_weights, device=self.device)\n","        if not batched:\n","            output_sequence = output_sequence.squeeze(0)\n","        return output_sequence, attention_weights\n","        \n","    def training_step(self, batch, batch_idx):\n","        input, target = batch\n","        \n","        self.encoder = self.encoder.to(self.device)\n","        self.decoder = self.decoder.to(self.device)\n","        \n","        batched = True if len(input.shape) > 1 else False\n","        if not batched:\n","            input = input.unsqueeze(0)\n","            target = target.unsqueeze(0)\n","        input = input.to(self.device)\n","        target = target.to(self.device)\n","        batch_size = input.shape[0]\n","        input_length = input.shape[1]\n","        target_length = target.shape[1]\n","\n","        encoder_hidden = None\n","        encoder_hidden_outputs = torch.zeros(input_length, self.encoder.direction, batch_size, self.encoder.hidden_size, device=self.device)\n","        encoder_output_gate = torch.zeros(input_length, self.encoder.direction, batch_size, self.encoder.hidden_size, device=self.device)\n","        if self.encoder.cell_type == 'LSTM':\n","            a, b = [torch.zeros(self.encoder.direction, batch_size, self.encoder.hidden_size)]*2\n","            encoder_hidden = a.to(self.device), b.to(self.device)\n","        else:\n","            encoder_hidden = torch.zeros(self.encoder.direction, batch_size, self.encoder.hidden_size).to(self.device)\n","        for i in range(input_length):\n","            # print(input[:, i].shape, encoder_hidden.shape)\n","            _, encoder_hidden_out = self.encoder(input[:, i].unsqueeze(1), encoder_hidden)\n","            encoder_hidden = encoder_hidden_out\n","            if self.encoder.cell_type == 'LSTM':\n","                encoder_hidden_outputs[i] = encoder_hidden_out[0]\n","                encoder_output_gate[i] = encoder_hidden_out[1]\n","            else:\n","                encoder_hidden_outputs[i] = encoder_hidden_out\n","        loss = 0\n","        correct_words = 0\n","        if self.encoder.cell_type == 'LSTM':\n","            decoder_hidden = encoder_hidden_outputs[-1], encoder_output_gate[-1]\n","        else:\n","            decoder_hidden = encoder_hidden_outputs[-1]\n","        if random.random() < 0.5: \n","            decoder_input = target[:, 0].unsqueeze(1)\n","            correct = None\n","            for j in range(target_length):\n","                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_hidden_outputs)\n","                squeezed_output = decoder_output.squeeze(1)\n","                for i in range(batch_size):\n","                    loss += nn.functional.nll_loss(squeezed_output[i], target[i, j])\n","                decoder_input = target[:, j].unsqueeze(1)\n","                if correct is None:\n","                    correct = decoder_output.argmax(dim=-1) == target[:, j]\n","                else:\n","                    correct = (decoder_output.argmax(dim=-1) == target[:, j]) & correct\n","            correct_words = correct.sum()\n","\n","        else:\n","            decoder_input = target[:, 0].unsqueeze(1)\n","            correct = None\n","            for j in range(target_length):\n","                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_hidden_outputs)\n","                squeezed_output = decoder_output.squeeze(1)\n","                for i in range(batch_size):\n","                    loss += nn.functional.nll_loss(squeezed_output[i], target[i, j])\n","                decoder_input = decoder_output.argmax(dim=-1)\n","                if correct is None:\n","                    correct = decoder_input == target[:, j]\n","                else:\n","                    correct = (decoder_input == target[:, j]) & correct\n","            correct_words = correct.sum()\n","\n","        # print(correct_words, batch_size, correct_words/batch_size)\n","        reported_loss = loss / (batch_size * target_length)\n","        self.log('train_loss', reported_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('train_acc', correct_words/batch_size, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n","        return loss\n","    def validation_step(self, batch, batch_idx):\n","        input, target = batch\n","        \n","        self.encoder = self.encoder.to(self.device)\n","        self.decoder = self.decoder.to(self.device)\n","        \n","        batched = True if len(input.shape) > 1 else False\n","        if not batched:\n","            input = input.unsqueeze(0)\n","            target = target.unsqueeze(0)\n","        input = input.to(self.device)\n","        target = target.to(self.device)\n","        batch_size = input.shape[0]\n","        input_length = input.shape[1]\n","        target_length = target.shape[1]\n","\n","        encoder_hidden = None\n","        encoder_hidden_outputs = torch.zeros(input_length, self.encoder.direction, batch_size, self.encoder.hidden_size, device=self.device)\n","        encoder_output_gate = torch.zeros(input_length, self.encoder.direction, batch_size, self.encoder.hidden_size, device=self.device)\n","        if self.encoder.cell_type == 'LSTM':\n","            a, b = [torch.zeros(self.encoder.direction, batch_size, self.encoder.hidden_size)]*2\n","            encoder_hidden = a.to(self.device), b.to(self.device)\n","        else:\n","            encoder_hidden = torch.zeros(self.encoder.direction, batch_size, self.encoder.hidden_size).to(self.device)\n","        for i in range(input_length):\n","            # print(input[:, i].shape, encoder_hidden.shape)\n","            _, encoder_hidden_out = self.encoder(input[:, i].unsqueeze(1), encoder_hidden)\n","            encoder_hidden = encoder_hidden_out\n","            if self.encoder.cell_type == 'LSTM':\n","                encoder_hidden_outputs[i] = encoder_hidden_out[0]\n","                encoder_output_gate[i] = encoder_hidden_out[1]\n","            else:\n","                encoder_hidden_outputs[i] = encoder_hidden_out\n","        loss = 0\n","        correct_words = 0\n","        if self.encoder.cell_type == 'LSTM':\n","            decoder_hidden = encoder_hidden_outputs[-1], encoder_output_gate[-1]\n","        else:\n","            decoder_hidden = encoder_hidden_outputs[-1]\n","        decoder_input = target[:, 0].unsqueeze(1)\n","        correct = None\n","        for j in range(target_length):\n","            decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_hidden_outputs)\n","            squeezed_output = decoder_output.squeeze(1)\n","            for i in range(batch_size):\n","                loss += nn.functional.nll_loss(squeezed_output[i], target[i, j])\n","            decoder_input = decoder_output.argmax(dim=-1)\n","            if correct is None:\n","                correct = decoder_input == target[:, j]\n","            else:\n","                correct = (decoder_input == target[:, j]) & correct\n","        correct_words = correct.sum()\n","\n","        # for i in range(batch_size):\n","        #     if self.encoder.cell_type == 'LSTM':\n","        #         decoder_hidden = encoder_hidden_outputs[i].view(self.decoder.direction, -1), encoder_output_gate[i].view(self.decoder.direction, -1)\n","        #     else:\n","        #         decoder_hidden = encoder_hidden_outputs[i].view(self.decoder.direction, -1)\n","        #     decoder_input = target[i, 0].unsqueeze(0)\n","        #     correct = True\n","        #     for j in range(target_length):\n","        #         decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n","        #         loss += nn.functional.nll_loss(decoder_output, target[i, j])\n","        #         decoder_input = torch.tensor([decoder_output.argmax().item()]).to(self.device)\n","        #         if correct and target[i, j]!= decoder_output.argmax().item():\n","        #             correct = False\n","        #     if correct:\n","        #         correct_words  += 1\n","        reported_loss = loss / (batch_size * target_length)\n","        self.log('val_loss', reported_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('val_acc', correct_words/batch_size, on_epoch=True, prog_bar=True, logger=True)\n","        return loss\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["encoder = Encoder(30, 128, cell_type=\"LSTM\", num_layers=3, bidirectional=True, dropout=0.1)\n","decoder = AttnDecoder(150, 128, 144, cell_type=\"LSTM\", num_layers=1, bidirectional=True)\n","model = AttnSeq2seq(encoder, decoder)"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","\n","  | Name    | Type        | Params\n","----------------------------------------\n","0 | encoder | Encoder     | 663 K \n","1 | decoder | AttnDecoder | 445 K \n","----------------------------------------\n","1.1 M     Trainable params\n","0         Non-trainable params\n","1.1 M     Total params\n","4.435     Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: 0it [00:00, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["/home/sooraj/.local/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["                                                                           \r"]},{"name":"stderr","output_type":"stream","text":["/home/sooraj/.local/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4:   5%|▌         | 83/1599 [00:46<14:09,  1.78it/s, v_num=231, train_loss_step=0.502, train_acc_step=0.281, val_loss_step=0.357, val_loss_epoch=0.381, val_acc=0.308, train_loss_epoch=0.334, train_acc_epoch=0.267]     "]},{"name":"stderr","output_type":"stream","text":["/home/sooraj/.local/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n","  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"]}],"source":["trainer = pl.Trainer(max_epochs=10)\n","trainer.fit(model, train_loader)"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["torch.save(model.state_dict(), 'model.pt')"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["model.load_state_dict(torch.load('model.pt'))"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"data":{"text/plain":["'SOSघररEOSPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD'"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["output, attention = model(convert_word_to_tensor('ghar', 'eng'))\n","convert_tensor_to_word(output, 'hin')"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":["attention = [x.detach().numpy() for x in attention]"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[],"source":["attention = np.array(attention)"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["attention = attention.squeeze()"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fcf3f783f50>"]},"execution_count":103,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAaMAAAGkCAYAAACckEpMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaD0lEQVR4nO3db2xUdb7H8c+Z0k4rtMNWaKeVgkVUdlW6uSi1UTcqDW0fEFFyA8QHhUvcXLc1wV5DQrL8MWvS6CYucdOF3NxdWB6ALg/A6AM2bpVyN0sx4iW75u7tAumGcqFl5aYdqHZaOr/7gOvsHUFh5kz7nT/vV3KinTm//r78ejqfnvbM+XrOOScAAAwFrAsAAIAwAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJjLujDq6urS3XffreLiYtXX1+vjjz+2Lilj7dixQ57nJWyLFy+2LivjHDt2TCtXrlR1dbU8z9Phw4cTnnfOadu2baqqqlJJSYkaGxt1+vRpm2IzxK3WbP369Tcce83NzTbFZojOzk498sgjKi0tVUVFhVatWqW+vr6EfcbGxtTW1qY777xTs2bN0urVqzU0NGRU8fTKqjB655131NHRoe3bt+vTTz9VXV2dmpqadOnSJevSMtYDDzygixcvxrff//731iVlnNHRUdXV1amrq+umz7/xxht66623tHv3bp04cUIzZ85UU1OTxsbGprnSzHGrNZOk5ubmhGPvwIED01hh5unp6VFbW5t6e3v1wQcfaGJiQitWrNDo6Gh8n5dfflnvvfeeDh48qJ6eHl24cEHPPfecYdXTyGWRZcuWuba2tvjHk5OTrrq62nV2dhpWlbm2b9/u6urqrMvIKpLcoUOH4h/HYjEXDofdT3/60/hjw8PDLhgMugMHDhhUmHm+vmbOOdfa2uqeeeYZk3qyxaVLl5wk19PT45y7flwVFha6gwcPxvf585//7CS548ePW5U5bbLmzGh8fFwnT55UY2Nj/LFAIKDGxkYdP37csLLMdvr0aVVXV2vhwoV6/vnnde7cOeuSskp/f78GBwcTjrtQKKT6+nqOu1s4evSoKioqdP/99+vFF1/U5cuXrUvKKCMjI5Kk8vJySdLJkyc1MTGRcKwtXrxY8+fPz4tjLWvC6PPPP9fk5KQqKysTHq+srNTg4KBRVZmtvr5ee/fu1ZEjR7Rr1y719/friSee0JUrV6xLyxpfHVscd8lpbm7Wvn371N3drddff109PT1qaWnR5OSkdWkZIRaLadOmTXrsscf04IMPSrp+rBUVFWn27NkJ++bLsTbDugBMnZaWlvj/L1myRPX19VqwYIF+85vfaOPGjYaVIdetXbs2/v8PPfSQlixZonvuuUdHjx7V8uXLDSvLDG1tbfrss8/4G+7/kzVnRnPmzFFBQcENV5YMDQ0pHA4bVZVdZs+erfvuu09nzpyxLiVrfHVscdz5s3DhQs2ZM4djT1J7e7vef/99ffTRR5o3b1788XA4rPHxcQ0PDyfsny/HWtaEUVFRkZYuXaru7u74Y7FYTN3d3WpoaDCsLHtcvXpVZ8+eVVVVlXUpWaO2tlbhcDjhuItEIjpx4gTHXRLOnz+vy5cv5/Wx55xTe3u7Dh06pA8//FC1tbUJzy9dulSFhYUJx1pfX5/OnTuXF8daVv2arqOjQ62trXr44Ye1bNky7dy5U6Ojo9qwYYN1aRnplVde0cqVK7VgwQJduHBB27dvV0FBgdatW2ddWka5evVqwk/s/f39OnXqlMrLyzV//nxt2rRJr732mu69917V1tZq69atqq6u1qpVq+yKNvZta1ZeXq5XX31Vq1evVjgc1tmzZ7V582YtWrRITU1NhlXbamtr0/79+/Xuu++qtLQ0/negUCikkpIShUIhbdy4UR0dHSovL1dZWZleeuklNTQ06NFHHzWufhpYX86XrJ///Odu/vz5rqioyC1btsz19vZal5Sx1qxZ46qqqlxRUZG766673Jo1a9yZM2esy8o4H330kZN0w9ba2uqcu35599atW11lZaULBoNu+fLlrq+vz7ZoY9+2Zl988YVbsWKFmzt3rissLHQLFixwL7zwghscHLQu29TN1kuS27NnT3yfL7/80v3oRz9y3/nOd9wdd9zhnn32WXfx4kW7oqeR55xz0x+BAAD8Xdb8zQgAkLsIIwCAOcIIAGCOMAIAmCOMAADmCCMAgLmsC6NoNKodO3YoGo1al5JVWLfksWapYd2Sx5pJWfc+o0gkolAopJGREZWVlVmXkzVYt+SxZqlh3ZLHmmXhmREAIPcQRgAAcxl3o9RYLKYLFy6otLRUnufd8HwkEkn4L24P65Y81iw1rFvycnXNnHO6cuWKqqurFQh8+7lPxv3N6Pz586qpqbEuAwCQJgMDAwm9m24m486MSktLJUk/KFmtGV5h0uO9cIWv+c/8U+Wtd/oGs/t8Ta057/1XymNjo2P+JvfBZXEraS9w49l3XvD4DX1eMTrOr7kJ/fv4ofjr+rfJuDD66ldzM7xCzfCKkh9fEPQ1f6C4OOWxBcmXmyCVf+9XYp5dILgsfmG72a+C80IWf82QAuPj/Ha+zzgiAQDmpiyMurq6dPfdd6u4uFj19fX6+OOPp2oqAECWm5Iweuedd9TR0aHt27fr008/VV1dnZqamnTp0qWpmA4AkOWmJIzefPNNvfDCC9qwYYO+973vaffu3brjjjv0q1/9aiqmAwBkubSH0fj4uE6ePKnGxsa/TxIIqLGxUcePH79h/2g0qkgkkrABAPJL2sPo888/1+TkpCorEy+Rrqys1ODg4A37d3Z2KhQKxTfeYwQA+cf8arotW7ZoZGQkvg0MDFiXBACYZml/n9GcOXNUUFCgoaGhhMeHhoYUDodv2D8YDCoY9PfeIABAdkv7mVFRUZGWLl2q7u7u+GOxWEzd3d1qaGhI93QAgBwwJXdg6OjoUGtrqx5++GEtW7ZMO3fu1OjoqDZs2DAV0wEAstyUhNGaNWv0t7/9Tdu2bdPg4KC+//3v68iRIzdc1AAAgDSF96Zrb29Xe3v7VH16AEAOybgbpX4l9sWXinnXkh731/U3XiSRjH9o+EvKY0e7Zvqa+9qIj/dYZVYnkKzhYtYVALnLuYnb3tf80m4AAAgjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYC5j+xml6u5tH/saf6WkOPXBiyp8zf3b//6PlMc2zVvqa25fjX0seyl5nt3cAG7Bk27z5YEzIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGAu51pIKDbpb/gXX6Q++E9/8TX3ogP/nPLYO17293PFXf/6p5THuvFxX3Nr0sfXzMvTn6cCtM7A7fOMWq0EnCeN3ea+U1sKAAC3RhgBAMwRRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHOEEQDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMwRRgAAc5nbz8jzrm/Jcs7fvH7GO3+9lO75l96Ux365apmvua/9w6KUxxadveRrbjd2mw1PbmYy5mtuUy6La0d2Mer75bkC+hkBALIHYQQAMEcYAQDMEUYAAHOEEQDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMwRRgAAc4QRAMAcYQQAMJexLSS8GYXyvMKkx7mJ8SmoJvPNOtrna/xE3cKUx7rQLF9ze6m0Cvlq7nGfX+9Jf20/fHGp/7uBrBC7/WOcMyMAgDnCCABgjjACAJhLexjt2LFDnuclbIsXL073NACAHDIlFzA88MAD+t3vfvf3SWZk7HUSAIAMMCUpMWPGDIXD4an41ACAHDQlfzM6ffq0qqurtXDhQj3//PM6d+7cN+4bjUYViUQSNgBAfkl7GNXX12vv3r06cuSIdu3apf7+fj3xxBO6cuXKTffv7OxUKBSKbzU1NekuCQCQ4TznnJvKCYaHh7VgwQK9+eab2rhx4w3PR6NRRaPR+MeRSEQ1NTV6qvAfNYM3vd62gtkhX+P9vOm18PMvfM3tRUZTHpvdb3qd0m89wNy12Li6/2evRkZGVFZW9q37TvmVBbNnz9Z9992nM2fO3PT5YDCoYDA41WUAADLYlL/P6OrVqzp79qyqqqqmeioAQJZKexi98sor6unp0V//+lf94Q9/0LPPPquCggKtW7cu3VMBAHJE2n9Nd/78ea1bt06XL1/W3Llz9fjjj6u3t1dz585N91QAgByR9jB6++230/0pAQA5jnvTAQDMZex9ety1Cdq9JGFyeMTX+ILe/0x98C0u2bylmSUpD/WKivzN7efy6ny9NDsWs64gO+Xh8eLFbv/fzJkRAMAcYQQAMEcYAQDMEUYAAHOEEQDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMwRRgAAc4QRAMAcYQQAMJexLSQwzSYnUx87Me5vblfsY6zhbfm9LO5x4mfdAoY/w2Zz+wrL4yUL2ldwZgQAMEcYAQDMEUYAAHOEEQDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMwRRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHMZ28/IKyiQ5xUkPc5du+Zv3hmpL4nz0xNIMu054mKpz+3GJ3zN7fkZ77e3Tjb3JPLDz7/bsjeO3693NvdDynGcGQEAzBFGAABzhBEAwBxhBAAwRxgBAMwRRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHOEEQDAHGEEADBHGAEAzBFGAABzGdtCQgUFUgotJOSjFYIkeSUlqQ8ei/qa202Mpz7YZysEL2DXSsH5uK2/57edgZ91K0jh+MwUlm0gsrWNg+Wa5QHOjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYC5j+xl5BQF5Fv2MiotTHzwx4Wtu52+4Hb/9aSYnUx7qfPYU8tXFKV/721j2I8riNXdZXHuqkvk3c2YEADBHGAEAzBFGAABzSYfRsWPHtHLlSlVXV8vzPB0+fDjheeectm3bpqqqKpWUlKixsVGnT59OV70AgByUdBiNjo6qrq5OXV1dN33+jTfe0FtvvaXdu3frxIkTmjlzppqamjQ2Nua7WABAbkr6arqWlha1tLTc9DnnnHbu3Kkf//jHeuaZZyRJ+/btU2VlpQ4fPqy1a9f6qxYAkJPS+jej/v5+DQ4OqrGxMf5YKBRSfX29jh8/ftMx0WhUkUgkYQMA5Je0htHg4KAkqbKyMuHxysrK+HNf19nZqVAoFN9qamrSWRIAIAuYX023ZcsWjYyMxLeBgQHrkgAA0yytYRQOhyVJQ0NDCY8PDQ3Fn/u6YDCosrKyhA0AkF/SGka1tbUKh8Pq7u6OPxaJRHTixAk1NDSkcyoAQA5J+mq6q1ev6syZM/GP+/v7derUKZWXl2v+/PnatGmTXnvtNd17772qra3V1q1bVV1drVWrVqWzbgBADkk6jD755BM99dRT8Y87OjokSa2trdq7d682b96s0dFR/fCHP9Tw8LAef/xxHTlyRMV+bkAKAMhpnsuwW8lGIhGFQiE9fcdazfCKkh7vJq75mj8wO5TyWHfliq+5Y37eGOz5uv+0PB93v/Zm+Lv5uzdrZuqD/d6128+6+Zw7a3HX7pRk2EvttLgWG1f3pX/TyMjILa8HyNwWEkWF8rzCFAb6e1HW7NKUh3rX/AWhotHUx3rmF0ambtLuxc1PCwrPR+uLfJa1L8o+29PkpSTWLItfwQAAuYIwAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYyt59RsFheIPnmevLGfc3rZqbekTZwJYV6c4Df/jTZ2hfIZXMPqXzlDBsD5qMk1pvvJgCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGAuY1tIqHCGFChMflzUXwsJXzzP5/j8/NnATwsKnysu+Wp/4bMdQbZ+vWnDkF/8fI8kMTZLvxsAALmEMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmMrefUUFACqSQlQF/HW6cn55EqdSbKfK2t05BWspICX2BcLt89d3KDln6CgQAyCWEEQDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMwRRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHOEEQDAXOa2kACAXJIHbSD84MwIAGCOMAIAmCOMAADmkg6jY8eOaeXKlaqurpbneTp8+HDC8+vXr5fneQlbc3NzuuoFAOSgpMNodHRUdXV16urq+sZ9mpubdfHixfh24MABX0UCAHJb0lfTtbS0qKWl5Vv3CQaDCofDKRcFAMgvU/I3o6NHj6qiokL333+/XnzxRV2+fPkb941Go4pEIgkbACC/pD2MmpubtW/fPnV3d+v1119XT0+PWlpaNDk5edP9Ozs7FQqF4ltNTU26SwIAZLi0v+l17dq18f9/6KGHtGTJEt1zzz06evSoli9ffsP+W7ZsUUdHR/zjSCRCIAFAnpnyS7sXLlyoOXPm6MyZMzd9PhgMqqysLGEDAOSXKQ+j8+fP6/Lly6qqqprqqQAAWSrpX9NdvXo14Synv79fp06dUnl5ucrLy/Xqq69q9erVCofDOnv2rDZv3qxFixapqakprYUDAHJH0mH0ySef6Kmnnop//NXfe1pbW7Vr1y798Y9/1K9//WsNDw+rurpaK1as0E9+8hMFg8H0VQ0AyClJh9GTTz4p9y13n/3tb3/rqyAAQP7h3nQAAHOZ28/I865vuC1ewG6tPL5OyAf0I5pSnBkBAMwRRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHOEEQDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMwRRgAAcxnbQsIFAnIFKWSlZTsDWikAU4s2DjmLMyMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJjL2H5GCgSub9M+7/RPCUMxH/1x/B4rlv2v6AuEDMNLLwDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMwRRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHOEEQDAHGEEADCXuS0kCgLXt2R5PvPVz239LVsCIPvQxgHZwE+blSSOcc6MAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgLnP7GXleav2BAj57CtGTaPrFYqmPTaXnFWDBT1+gPMB3MgDAHGEEADBHGAEAzCUVRp2dnXrkkUdUWlqqiooKrVq1Sn19fQn7jI2Nqa2tTXfeeadmzZql1atXa2hoKK1FAwByS1Jh1NPTo7a2NvX29uqDDz7QxMSEVqxYodHR0fg+L7/8st577z0dPHhQPT09unDhgp577rm0Fw4AyB1JXU135MiRhI/37t2riooKnTx5Uj/4wQ80MjKiX/7yl9q/f7+efvppSdKePXv03e9+V729vXr00UfTVzkAIGf4+pvRyMiIJKm8vFySdPLkSU1MTKixsTG+z+LFizV//nwdP378pp8jGo0qEokkbACA/JJyGMViMW3atEmPPfaYHnzwQUnS4OCgioqKNHv27IR9KysrNTg4eNPP09nZqVAoFN9qampSLQkAkKVSDqO2tjZ99tlnevvtt30VsGXLFo2MjMS3gYEBX58PAJB9UroDQ3t7u95//30dO3ZM8+bNiz8eDoc1Pj6u4eHhhLOjoaEhhcPhm36uYDCoYDCYShkAgByR1JmRc07t7e06dOiQPvzwQ9XW1iY8v3TpUhUWFqq7uzv+WF9fn86dO6eGhob0VAwAyDlJnRm1tbVp//79evfdd1VaWhr/O1AoFFJJSYlCoZA2btyojo4OlZeXq6ysTC+99JIaGhq4kg4A8I2SCqNdu3ZJkp588smEx/fs2aP169dLkn72s58pEAho9erVikajampq0i9+8Yu0FAsAyE1JhZFzt77rbHFxsbq6utTV1ZVyUQCA/JK5LSQAIJPQAmJKcaNUAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYo5/R1zjPugIAU4aeRBmLMyMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgLnNbSHje9Q25L8DPRLhNtIDIWbwKAADMEUYAAHOEEQDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMwRRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHMZ10LCueu3iL82GU3tE8TGfc0/eW0s9cGxFGv+P9fcRMpjPee33UbqP5f4ntul3hbAc35/nvIxPkaLk2nn41jB9Lvmrr8eu9v4unnudvaaRufPn1dNTY11GQCANBkYGNC8efO+dZ+MC6NYLKYLFy6otLRU3k2a60UiEdXU1GhgYEBlZWUGFWYn1i15rFlqWLfk5eqaOed05coVVVdXK3CLJpoZ92u6QCBwywSVpLKyspz6ok0X1i15rFlqWLfk5eKahUKh29qPCxgAAOYIIwCAuawLo2AwqO3btysYDFqXklVYt+SxZqlh3ZLHmmXgBQwAgPyTdWdGAIDcQxgBAMwRRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHOEEQDA3P8CeUSz6NiWD0EAAAAASUVORK5CYII=","text/plain":["<Figure size 480x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.matshow(attention)"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["sweep_attn_config = {\n","    'method': 'bayes',\n","    'metric': {\n","        'name': 'val_loss',\n","        'goal': 'minimize'\n","    },\n","    'parameters': {\n","        'hidden_size': {\n","            'values': [64, 128, 256],\n","        },\n","        'encoder_num_layers': {\n","            'values': [1, 2, 3],\n","        },\n","        'bidirectional': {\n","            'values': [True, False],\n","        },\n","        'cell_type': {\n","            'values': ['LSTM', 'GRU'],\n","        },\n","    }\n","}"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["def sweep_attn_fn():\n","    wandb.init()\n","    config = wandb.config\n","    dropout_val = 0\n","    encoder=Encoder(30, config.hidden_size, config.cell_type, num_layers=config.encoder_num_layers, bidirectional=config.bidirectional)\n","    decoder = AttnDecoder(150, config.hidden_size, 64, config.cell_type, num_layers=1, bidirectional=config.bidirectional)\n","    model = AttnSeq2seq(encoder, decoder)\n","    logger = WandbLogger(project='CS6910 Assignment 3', entity='cs20b075')\n","    trainer = pl.Trainer(max_epochs=5, precision=16, logger=logger)\n","    trainer.fit(model, train_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Create sweep with ID: 1t3u8y0a\n","Sweep URL: https://wandb.ai/cs20b075/CS6910%20Assignment%203/sweeps/1t3u8y0a\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ai7hogtt with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_num_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.15.3 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/sooraj/sem/dl/assignment-3/wandb/run-20230518_221638-ai7hogtt</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cs20b075/CS6910%20Assignment%203/runs/ai7hogtt' target=\"_blank\">lilac-sweep-1</a></strong> to <a href='https://wandb.ai/cs20b075/CS6910%20Assignment%203' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b075/CS6910%20Assignment%203/sweeps/1t3u8y0a' target=\"_blank\">https://wandb.ai/cs20b075/CS6910%20Assignment%203/sweeps/1t3u8y0a</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cs20b075/CS6910%20Assignment%203' target=\"_blank\">https://wandb.ai/cs20b075/CS6910%20Assignment%203</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/cs20b075/CS6910%20Assignment%203/sweeps/1t3u8y0a' target=\"_blank\">https://wandb.ai/cs20b075/CS6910%20Assignment%203/sweeps/1t3u8y0a</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cs20b075/CS6910%20Assignment%203/runs/ai7hogtt' target=\"_blank\">https://wandb.ai/cs20b075/CS6910%20Assignment%203/runs/ai7hogtt</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/sooraj/.local/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","  rank_zero_warn(\n","/home/sooraj/.local/lib/python3.11/site-packages/lightning/fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n","  rank_zero_warn(\n","/home/sooraj/.local/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:517: UserWarning: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n","  rank_zero_warn(\n","Using bfloat16 Automatic Mixed Precision (AMP)\n","GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","\n","  | Name    | Type        | Params\n","----------------------------------------\n","0 | encoder | Encoder     | 1.1 M \n","1 | decoder | AttnDecoder | 767 K \n","----------------------------------------\n","1.8 M     Trainable params\n","0         Non-trainable params\n","1.8 M     Total params\n","7.312     Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: 0it [00:00, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["/home/sooraj/.local/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["                                                                           \r"]},{"name":"stderr","output_type":"stream","text":["/home/sooraj/.local/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0:  42%|████▏     | 666/1599 [05:34<07:48,  1.99it/s, v_num=ogtt, train_loss_step=0.951, train_acc_step=0.000]"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0:  60%|██████    | 960/1599 [08:19<05:32,  1.92it/s, v_num=ogtt, train_loss_step=1.070, train_acc_step=0.000]"]}],"source":["sweep_id = wandb.sweep(sweep=sweep_attn_config, project=\"CS6910 Assignment 3\")\n","wandb.agent(sweep_id=sweep_id, function=sweep_attn_fn, count=10)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Testing the models"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The best model found for seq2seq without attention was:\n","- bidirectional LSTM\n","- 3 decoder layers\n","- 2 encoder layers\n","- 200 hidden layer size\n","\n","While these results are found for 5 epochs, we shall be checking them against 10 epochs for the test set."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["encoder  = Encoder(30, 200, cell_type=\"LSTM\", num_layers=2, bidirectional=True, dropout=0.1)\n","decoder = Decoder(150, 200, cell_type=\"LSTM\", num_layers=3, bidirectional=True)\n","model = Seq2seq(encoder, decoder)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","\n","  | Name    | Type    | Params\n","------------------------------------\n","0 | encoder | Encoder | 1.6 M \n","1 | decoder | Decoder | 1.7 M \n","------------------------------------\n","3.3 M     Trainable params\n","0         Non-trainable params\n","3.3 M     Total params\n","13.236    Total estimated model params size (MB)\n","2023-05-20 10:25:09.624118: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-05-20 10:25:09.772983: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-20 10:25:10.660666: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: 0it [00:00, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["/home/sooraj/.local/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["                                                                           \r"]},{"name":"stderr","output_type":"stream","text":["/home/sooraj/.local/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9: 100%|██████████| 1599/1599 [16:52<00:00,  1.58it/s, v_num=232, train_loss_step=0.367, train_acc_step=0.462, val_loss_step=0.283, val_loss_epoch=0.392, val_acc=0.307, train_loss_epoch=0.268, train_acc_epoch=0.370]    "]},{"name":"stderr","output_type":"stream","text":["`Trainer.fit` stopped: `max_epochs=10` reached.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9: 100%|██████████| 1599/1599 [16:52<00:00,  1.58it/s, v_num=232, train_loss_step=0.367, train_acc_step=0.462, val_loss_step=0.283, val_loss_epoch=0.392, val_acc=0.307, train_loss_epoch=0.268, train_acc_epoch=0.370]\n"]}],"source":["trainer = pl.Trainer(max_epochs=10)\n","trainer.fit(model, train_loader)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["torch.save(model.state_dict(), 'seq2seqmodel.pt')"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m convert_tensor_to_word(model(convert_word_to_tensor(\u001b[39m'\u001b[39;49m\u001b[39mgharelu\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39meng\u001b[39;49m\u001b[39m'\u001b[39;49m)), \u001b[39m'\u001b[39m\u001b[39mhin\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[12], line 42\u001b[0m, in \u001b[0;36mSeq2seq.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     40\u001b[0m decoder_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(batch_size, \u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(input_length):\n\u001b[0;32m---> 42\u001b[0m     decoder_output, decoder_hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(decoder_input, decoder_hidden)\n\u001b[1;32m     43\u001b[0m     decoder_input \u001b[39m=\u001b[39m decoder_output\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m output_sequence \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(output_sequence, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[11], line 22\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     20\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(\u001b[39minput\u001b[39m)\n\u001b[1;32m     21\u001b[0m output \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(output)\n\u001b[0;32m---> 22\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfirst_cell(output, hidden)\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     24\u001b[0m     output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnns[i](output, hidden)\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/rnn.py:803\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    801\u001b[0m             msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mFor unbatched 2-D input, hx and cx should \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    802\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39malso be 2-D but got (\u001b[39m\u001b[39m{\u001b[39;00mhx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D, \u001b[39m\u001b[39m{\u001b[39;00mhx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D) tensors\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 803\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    804\u001b[0m         hx \u001b[39m=\u001b[39m (hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[1;32m    806\u001b[0m \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"]}],"source":[]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([24])"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["convert_word_to_tensor('ghar', 'eng').shape"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
